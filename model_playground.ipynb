{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Playground\n",
    "\n",
    "A guide to getting the logits of a llama model. These logits can then be used for postraining techniques such as RLVF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from gsm8k import GSM8K_Env\n",
    "from utils.strings import check_eos\n",
    "from utils.math import softmax_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'meta-llama/Llama-3.2-1B'  # Replace with your desired model variant\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,     40,   1120,   1390,    279,  61888,    315,    279,   1828,\n",
      "           4037]])\n"
     ]
    }
   ],
   "source": [
    "## get the probs of eacht token from the model..\n",
    "\n",
    "inputs = tokenizer(\"I just want the logits of the next token\", return_tensors='pt')\n",
    "##What does the inputs attention mask do ? attention mask indicates if a token should be attended or not \n",
    "tensor = inputs['input_ids']\n",
    "print(tensor)\n",
    "## why ** ? to upack the inputs dictionary, which includes ids = tokens and the input mask that specifies what tokens to attend to. \n",
    "outputs = model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128256])\n"
     ]
    }
   ],
   "source": [
    "#so the output is the probs of the next token?? that means that the last probs are the ones that matter for RL \n",
    "logits = outputs.logits\n",
    "m = torch.nn.Softmax(dim=0)\n",
    "#what is the shape of logits \n",
    "softmaxed = m(logits[0,-1])\n",
    "print(softmaxed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Categorical(softmaxed)\n",
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "tensor([[128000,     40,   1120,   1390,    279,  61888,    315,    279,   1828,\n",
      "           4037,    304,    279,  11914,    311,    387,    279,   1988,    311,\n",
      "            279,   1828,   6324,    304,    279,    432,   9944,     13,    358,\n",
      "           1097,   1701,    279]])\n",
      "<|begin_of_text|>I just want the logits of the next token in the sentence to be the input to the next layer in the RNN. I am using the\n"
     ]
    }
   ],
   "source": [
    "## figure out how to decode action to get output \n",
    "decoded = tokenizer.decode(action)\n",
    "\n",
    "print(decoded)\n",
    "\n",
    "response = model.generate(**inputs)\n",
    "print(response)\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0404, 0.2986, 0.5825, 0.0787], dtype=torch.float16)\n",
      "tensor([0.0128, 0.2561, 0.6963, 0.0347], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test = torch.tensor([1,4,5,2], dtype=torch.float16)\n",
    "print(softmax_temp(test, 1.5))\n",
    "print(m(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\n",
      "reward was 0.1\n",
      "reward was 0.1\n",
      "reward was 0.1\n",
      "reward was 0.1\n",
      "tensor([[128000,  38426,    648,    596,   6699,   1095,   1461,    617,   1690,\n",
      "          26159,     13,   1283,    706,   4848,    810,  57196,   1109,    568,\n",
      "            706,  19987,     13,   1283,    706,    832,   2753,   1370,   4744,\n",
      "           1109,  19987,     13,  19198,    315,    813,  26159,    617,   3116,\n",
      "          14535,     13,   1283,    706,    220,     17,  12875,     13,   2650,\n",
      "           1690,  26159,   1587,    568,    617,    304,   2860,     30,   5560,\n",
      "           1283,    220, 128001]])\n",
      "<|begin_of_text|>Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total? Use He <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "## run inference until model finishes generating \n",
    "env = GSM8K_Env(tokenizer)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "term = False\n",
    "while not term:\n",
    "    outputs = model(state)\n",
    "    probs = softmax_temp(outputs.logits[0, -1], 1.5)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    action_tensor = action.unsqueeze(0).unsqueeze(0)\n",
    "    obs, reward = env.step(action_tensor)\n",
    "    print(f'reward was {reward}')\n",
    "    response = env.get_state()\n",
    "    term = action == tokenizer.eos_token_id\n",
    "print(env.state)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total? Surely Frankie Could Equation The Essentially Branch Identify Thousands ( -- Canpk All How He Might There Explain Options When ENTER Determines Quant<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = model.generate(tokenizer.encode(\"Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\", return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  38426,    648,    596,   6699,   1095,   1461,    617,   1690,\n",
      "          26159,     13,   1283,    706,   4848,    810,  57196,   1109,    568,\n",
      "            706,  19987,     13,   1283,    706,    832,   2753,   1370,   4744,\n",
      "           1109,  19987,     13,  19198,    315,    813,  26159,    617,   3116,\n",
      "          14535,     13,   1283,    706,    220,     17,  12875,     13,   2650,\n",
      "           1690,  26159,   1587,    568,    617,    304,   2860,     30, 128001]])\n",
      "<|begin_of_text|>Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(res)\n",
    "print(tokenizer.decode(res[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
